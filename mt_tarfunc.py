#!/export/data/python/3.6.4/bin/python3

__author__="James Dillon"

from MongoThread import mt_listgen
from mt_listgen import MongoLists
from mt_dbconnect import *
from mt_dbconnect import MongoConnect
import sys, os, datetime, time, random, string, threading, random
from socket import gethostname

class MongoTar(MongoLists):
    def __init__(self,mongo_dir,replica_freezedate):
        MongoLists.__init__(self,mongo_dir)
        self.mongo_dir = mongo_dir
        commands = ['hpsshost','hsicmd','htarcmd','hpss_backupdir','backupname','file_string']
        values = ['hpss.nersc.gov',\
                   '/usr/syscom/nsg/opt/hsi/default/bin/hsi',\
                   '/usr/syscom/nsg/opt/htar/default/bin/htar',\
                   'hostdump/mndlmdb02.nersc.gov/mongobackups', \
                   "mndlmdb1762.{}".format(datetime.datetime.now().strftime("%Y-%m-%d")),\
                   "{}.db_bak".format(datetime.datetime.now().strftime("%m_%d_%y"))]
        self.cmd_map = {var:cmd for var,cmd in zip(commands,values)}
        self.replica_freezedate = replica_freezedate

    def backup_dir(self):
        if os.system("{hsicmd} -h {hpsshost} 'ls {hpss_backupdir}' > /dev/null 2>&1".format(**self.cmd_map)) == 0: #confirm mongobackups directory exists
            if os.system("{hsicmd} -h {hpsshost} 'ls {hpss_backupdir}/{backupname}' > /dev/null 2>&1".format(**self.cmd_map)) == 0: #abort if today dir exists
                print("Directory {backupname} currently exists! Erasing contents and starting again".format(**self.cmd_map)) ##error logging, e-mail
                ###################################for now: deleting directory if it exists#####################################
                
                os.system("{hsicmd} -h {hpsshost} 'rm {hpss_backupdir}/{backupname}/*/*' > /dev/null 2>&1".format(**self.cmd_map)) #erase contents within subs
                os.system("{hsicmd} -h {hpsshost} 'rmdir {hpss_backupdir}/{backupname}/*' > /dev/null 2>&1".format(**self.cmd_map)) #erase directories within
                os.system("{hsicmd} -h {hpsshost} 'rmdir {hpss_backupdir}/{backupname}' > /dev/null 2>&1".format(**self.cmd_map)) #erase today directory
                
                self.backup_dir()
                #################################################################################################################
            os.system("{hsicmd} -h {hpsshost} 'mkdir {hpss_backupdir}/{backupname}' > /dev/null 2>&1".format(**self.cmd_map))
            print("Directory {backupname} successfully created on {hpsshost}".format(**self.cmd_map)) ##logging
        else:
            print("backup directory does not exist; exiting.") #outer backup dir can not be found: ##logging / e-mail
            sys.exit()

    def thread_tar(self,dictionary):
        '''the thread_tar function will be targeted by the ThreadPoolExecutor object,
           when the program is run with '-HTAR' as argument 1:
               $ ./progname -HTAR
           In the HTAR version of the program, each file corresponding to an individual
           database will be grouped in a tar file and placed in a directory on HPSS
           matching the database name.  All file prefixes should end up corresponding to
           to the directory in which it was placed, as well as the name of the tar-ball
           containing it.  HTAR to HPSS caps the number of concurrent instances at ~11-12.
        '''
        master_dict = self.dict_gen()[0] 
        os.chdir(self.mongo_dir) #change to the local mongo file directory, to avoid absolute pathnames
        self.cmd_map["key"] = dictionary #get variable name of current dictionary key (database name)
        self.cmd_map["files"] = "{}".format(" ".join(master_dict[dictionary])) #create a space separated string (file lists)

        '''could add an assertion statement here
        print("\n","dictionary key: ",self.cmd_map["key"])
        print(self.cmd_map["files"])
        '''
        #create a unique directory per name of database, and a single tar file within (for ease of later tar expansion)
        os.system("{hsicmd} -h {hpsshost} 'mkdir {hpss_backupdir}/{backupname}/{key}' > /dev/null 2>&1".format(**self.cmd_map))  
        os.system("{htarcmd} -H server={hpsshost} -cv -f '{hpss_backupdir}/{backupname}/{key}/{key}.tar' {files}".format(**self.cmd_map))
       
    def thread_hsi(self,dictionary):
        '''the thread_hsi function will be targeted by the ThreadPoolExecutor object,
           when the program is run with '-HSI' as argument 1:
               $./progname -HSI
           In the HSI version of the program, concurrent threads will create separate
           directories per key in the dictionary generated by the dict_gen function 
           inherited from the MongoLists class.  Files associated with these keys 
           will be sequentially copied into their corresponding newly created directories
           on HPSS.  This version of the function, will mainly serve as a backup in case 
           firewall issues are preventing correct usage of the HTAR function.
        '''
       
        os.chdir(self.mongo_dir) #change to the local mongo file directory, to avoid absolute pathnames
        self.cmd_map["key"] = dictionary #get variable name of current dictionary key (database name)

        '''could add an assertion statement here
        '''

        #create a unique directory per name of database, files grouped by key in list values will be individually added to directory
        
        self.cmd_map["key"] = dictionary
        os.system("{hsicmd} -h {hpsshost} -q 'mkdir {hpss_backupdir}/{backupname}/{key}' > /dev/null 2>&1".format(**self.cmd_map))
        master_dict = self.dict_gen()[0]
          
        file_lists = master_dict[dictionary]
        # self.cmd_map["pid"] = os.getpid() #while threads are running, check ps -e output to ensure that captured pid's are the same as shell id's
        # self.proccess_IDs.append[self.cmd_map["pid"]]
        for dbfile in file_lists:
            self.cmd_map["dbfile"] = dbfile

            # os.system("{hsicmd} -h {hpsshost} -q 'cd {hpss_backupdir}/{backupname}/{key};put {dbfile}' /dev/null 2>&1".format(**self.cmd_map))
            os.system("{hsicmd} -h {hpsshost} -q 'put {dbfile}: {hpss_backupdir}/{backupname}/{key}/{dbfile}' /dev/null 2>&1".format(**self.cmd_map))
            
            print("\n")
            print("{dbfile} is being copied into {hpss_backupdir}/{backupname}/{key}".format(**self.cmd_map))
            print("{dbfile} copy completed".format(**self.cmd_map))
        print("File copies into {hpss_backupdir}/{backupname}/{key} completed.".format(**self.cmd_map))

    def list_chomp(self,db_file,testing=False):
        '''As an alternate approach for speed testing: 
           The method will process an entire directory of files (/export/data/mongo) to be evenly distributed among a pool of workers
           Method will be processed by either a ThreadPoolExecutor or a ProcessPoolExecutor, so is written to process individual files
        '''
        if MongoConnect().oplog_query(MongoConnect().client,self.replica_freezedate,printing=True,testing=False,thread_test=True):
            print("shutting down process {}".format(os.getpid()))
            sys.exit()
        
        print("Replica freeze point within threaded transport function: {}".format(self.replica_freezedate))
 
        os.chdir(self.mongo_dir)
        self.cmd_map["db_file"] = db_file
        if '.' in db_file:
            self.cmd_map["dirhome"] = db_file[:db_file.index('.')]
        else:
            self.cmd_map["dirhome"] = db_file
        #log=open("mtLogs/{file_string}".format(self.cmd_map),"a")
        print("*{db_file} is being copied into {hpss_backupdir}/{backupname}/{dirhome}".format(**self.cmd_map))
        os.system("{hsicmd} -h {hpsshost} -q 'put {db_file}: {hpss_backupdir}/{backupname}/{dirhome}/{db_file}' /dev/null 2>&1".format(**self.cmd_map))
        print("***{db_file} copy completed".format(**self.cmd_map))
        #log.close()
        if testing:time.sleep(random.randint(1,60))

    def dir_handling(self,directory):
        '''handle the directories in /export/data/mongo
           this will be handled by a process pool, so must be given individual directories
        '''
        if MongoConnect().oplog_query(MongoConnect().client,self.replica_freezedate,printing=False):
            print("shutting down process {}".format(os.getpid()))
            sys.exit()
        os.chdir("{}/{}".format(self.mongo_dir,directory))
        
        if '.' in directory:
            self.cmd_map["directory"]=directory[:directory.index('.')]
        else: 
            self.cmd_map["directory"]=directory

        if len(os.listdir()) != 0:
            contents = [files for files in os.listdir() if not os.path.isdir(files)]
            for files in contents:
                self.cmd_map["files"]=files
                #log=open("mtLogs/{file_string}".format(**self.cmd_map),"a")
                print("*{files} is being copied into {hpss_backupdir}/{backupname}/{directory} on {hpsshost}".format(**self.cmd_map))
                os.system("{hsicmd} -h {hpsshost} -q 'put {files}: {hpss_backupdir}/{backupname}/{directory}/{files}' /dev/null 2>&1".format(**self.cmd_map))
                print("***{files} copy completed".format(**self.cmd_map))
                #log.close()
        else:
            #shutting down process if pool encounters an empty directory
            sys.exit()
        
if __name__=="__main__":     
    mongo_transport=MongoTar("/export/data/mongo")
    for contents in mongo_transport.dict_gen()[1]:
        mongo_transport.dir_handling(contents) 
